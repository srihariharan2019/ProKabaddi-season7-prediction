{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=5><b> Software Requirements: The below scripts are tested in Python version 3.6.1 and Seaborn version 0.7.1</b> </font><br>   \n",
    " \n",
    "# <font color='blue'>Hackathon: Pro Kabaddi League</font> <br>\n",
    "# <font color='blue'> Important Instructions: Execute this second </font>\n",
    "# <font color='blue'> _______________ </font>\n",
    "## <font color='blue'> Setup Working directory, copy files, execute scripts </font>\n",
    "<font color='blue'> <br>\n",
    "1) Copy input CSV files into your home/working directory <br>\n",
    "2) To perform web scraping, please copy the provided Chrome Driver into the working directory\n",
    "2) Copy this ipython (ipynb) notebook into your home/working directory <br>\n",
    "3) Ensure that the CSV files and this ipython notebook (ipynb) are in the home/working directory <br>\n",
    "4) Execute below scripts by sequence order </font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping from www.prokabaddi.com for Team and Player information using Selenium Driver and Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium  import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def invoke_main(url,path):\n",
    "    driver = webdriver.Chrome()\n",
    "    player_list=[]\n",
    "    driver.get(url)\n",
    "    if (path =='player'):\n",
    "        wait(driver, 600).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"player_Btn\"]')))\n",
    "# or just wait for a second for browser(driver) to change\n",
    "        driver.implicitly_wait(1)\n",
    "    elif (path == 'team'):\n",
    "         wait(driver, 600).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"team_Btn\"]')))\n",
    "# or just wait for a second for browser(driver) to change\n",
    "         driver.implicitly_wait(1)\n",
    "    else:\n",
    "        print(\"path not found\")\n",
    "    while True:\n",
    "        try:\n",
    "            driver.implicitly_wait(1)\n",
    "        #wait(driver, 600).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"load_more\"]')))\n",
    "            wait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"load_more\"]'))).click()\n",
    "            soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        except selenium.common.exceptions.TimeoutException:\n",
    "            break\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    driver.close();\n",
    "    return soup\n",
    "\n",
    "def invoke_class_func(soup,class_name):\n",
    "    if (class_name =='sipk-lb-detailBlock sipk-lb-team'):\n",
    "        player_profiles = soup.findAll('a', attrs={'class': 'sipk-lb-detailBlock sipk-lb-team', 'href': True})\n",
    "        list_rec=[]\n",
    "        for profile in player_profiles:\n",
    "            #print(profile.find('a')['href'])\n",
    "            #print(profile.find('a').contents[0])\n",
    "            profile_name = profile['href']\n",
    "            \n",
    "            profile_name = profile_name.split(\"/\",1)[1]\n",
    "            profile_name = profile_name.replace(\"teams/\",\"\")\n",
    "            profile_name = profile_name.split('-profile',1)[0]\n",
    "\n",
    "            #profile_name = profile_name.replace(\"\\n\",\"\")\n",
    "            #profile_name = profile_name.strip()\n",
    "            list_rec.append(profile_name)\n",
    "    else:\n",
    "        name_list = soup.findAll('div', attrs={'class': class_name})\n",
    "        list_rec=[]\n",
    "        for lst in name_list:\n",
    "            rec1 = lst.find('span').text\n",
    "            rec1 = rec1.replace(\"\\n\",\"\")\n",
    "            rec1 = rec1.strip()\n",
    "            list_rec.append(rec1)\n",
    "        \n",
    "    return list_rec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Working well on this\n",
    "url_array = [\"https://www.prokabaddi.com/stats/0-102-total-points-statistics\",\"https://www.prokabaddi.com/stats/0-21-successful-raids-statistics\",\"https://www.prokabaddi.com/stats/0-22-raid-points-statistics\",\"https://www.prokabaddi.com/stats/0-23-successful-tackles-statistics\",\"https://www.prokabaddi.com/stats/0-103-tackle-points-statistics\",\"https://www.prokabaddi.com/stats/0-138-avg-raid-points-statistics\",\"https://www.prokabaddi.com/stats/0-139-avg-tackle-points-statistics\",\"https://www.prokabaddi.com/stats/0-132-do-or-die-raid-points-statistics\",\"https://www.prokabaddi.com/stats/0-104-super-raids-statistics\",\"https://www.prokabaddi.com/stats/0-28-super-tackles-statistics\",\"https://www.prokabaddi.com/stats/0-100-super-10s-statistics\",\"https://www.prokabaddi.com/stats/0-101-high-5s-statistics\"]\n",
    "path = \"player\"\n",
    "    \n",
    "def getplayerInfo(url_array_new, season_num):\n",
    "    for url in url_array_new:\n",
    "        season = str(season_num)\n",
    "        soup = invoke_main(url,path)\n",
    "        name_list = invoke_class_func(soup,'sipk-lb-playerName')\n",
    "        matches_played = invoke_class_func(soup,'sipk-lb-detailBlock sipk-lb-matchedPlayed')\n",
    "        raid_points = invoke_class_func(soup,'sipk-lb-detailBlock sipk-lb-raidPoints')\n",
    "        teams = invoke_class_func(soup,'sipk-lb-detailBlock sipk-lb-team')\n",
    "        #name_list = soup.findAll('div', attrs={'class': 'sipk-lb-playerName'})\n",
    "        str1 = url\n",
    "        str1 = str1.replace('https://www.prokabaddi.com/stats/','')\n",
    "        str2 = str1.split('-')\n",
    "        str_final = str2[2] + '-' + str2[3]\n",
    "        file_str = str2[2] + '-' + str2[3] + '-' + str2[4]\n",
    "        file_name = 'prokabaddi' + '_' + 'player' + '_' + file_str +  '_' + season + '.csv'\n",
    "        df = \"df\" + \"_\" + str_final\n",
    "        df = pd.DataFrame(name_list,columns =['player_name'])\n",
    "        df['Season'] = season\n",
    "        df['matches_played'] = matches_played\n",
    "        df['team'] = teams\n",
    "        df[str_final] = raid_points\n",
    "        #write the result to the CSV file\n",
    "        file_name = 'prokabaddi' + '_' + str_final +  '_' + season + '.csv'\n",
    "        df.to_csv(file_name,index=False)\n",
    "\n",
    "for j in range(0,8):\n",
    "    if (j==5):\n",
    "        \n",
    "        url_array_new = [j.replace(\"/0-\",\"/8-\") for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)\n",
    "    elif (j==6):\n",
    "        \n",
    "        url_array_new = [j.replace(\"/0-\",\"/26-\") for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)\n",
    "    elif (j==7):\n",
    "        \n",
    "        url_array_new = [j.replace(\"/0-\",\"/49-\") for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)\n",
    "        \n",
    "    else:\n",
    "        str1 = \"/\" +str(j)+\"-\"\n",
    "        url_array_new = [j.replace(\"/0-\",str1) for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "url_array = [\"https://www.prokabaddi.com/stats/0-133-total-points-conceded-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-140-avg-points-scored-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-13-successful-raids-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-97-raid-points-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-98-avg-raid-points-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-15-successful-tackles-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-95-tackle-points-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-99-avg-tackle-points-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-134-super-raid-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-20-super-tackles-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-135-do-or-die-raid-points-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-136-all-outs-inflicted-statistics\",\n",
    "             \"https://www.prokabaddi.com/stats/0-137-all-outs-conceded-statistics\"]\n",
    "path = \"team\"\n",
    "    \n",
    "def getplayerInfo(url_array_new, season_num):\n",
    "    for url in url_array_new:\n",
    "            season = str(season_num)\n",
    "            str1 = url\n",
    "            str1 = str1.replace('https://www.prokabaddi.com/stats/','')\n",
    "            str2 = str1.split('-')\n",
    "            \n",
    "            str_final = str2[2] + '-' + str2[3] \n",
    "            file_str = str_final + '-' + str2[4]\n",
    "            file_name = 'prokabaddi' + '_' + 'team' + '_' + file_str +  '_' + season + '.csv'\n",
    "            isfile = os.path.isfile(file_name)\n",
    "            if(~isfile):\n",
    "                \n",
    "                soup1 = invoke_main(url,path)\n",
    "                if (soup1):\n",
    "                    name_list = invoke_class_func(soup1,'sipk-lb-playerName')\n",
    "                    matches_played = invoke_class_func(soup1,'sipk-lb-detailBlock sipk-lb-matchedPlayed')\n",
    "                    raid_points = invoke_class_func(soup1,'sipk-lb-detailBlock sipk-lb-raidPoints')\n",
    "        #name_list = soup.findAll('div', attrs={'class': 'sipk-lb-playerName'})\n",
    "            \n",
    "                    df = \"df\" + \"_\" + str_final\n",
    "                    df = pd.DataFrame(name_list,columns =['team_name'])\n",
    "                    df['Season'] = season\n",
    "                    df['matches_played'] = matches_played\n",
    "                    df[str_final] = raid_points\n",
    "            #write the result to the CSV file\n",
    "\n",
    "                    df.to_csv(file_name,index=False)\n",
    "\n",
    "for j in range(0,8):\n",
    "    if (j==5):\n",
    "        \n",
    "        url_array_new = [j.replace(\"/0-\",\"/8-\") for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)\n",
    "    elif (j==6):\n",
    "        \n",
    "        url_array_new = [j.replace(\"/0-\",\"/26-\") for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)\n",
    "    elif (j==7):\n",
    "        \n",
    "        url_array_new = [j.replace(\"/0-\",\"/49-\") for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)\n",
    "        \n",
    "    else:\n",
    "        str1 = \"/\" +str(j)+\"-\"\n",
    "        url_array_new = [j.replace(\"/0-\",str1) for j in url_array]\n",
    "        getplayerInfo(url_array_new,j)\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## End of Web Scraping for Team and Player data. Check the working directory for the CSV output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
