{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=5><b> Software Requirements: The below scripts are tested in Python version 3.6.1 and Seaborn version 0.7.1</b> </font><br>   \n",
    " \n",
    "# <font color='blue'>Hackathon: Pro Kabaddi League</font> <br>\n",
    "# <font color='blue'> Important Instructions: Execute this first </font>\n",
    "# <font color='blue'> _______________ </font>\n",
    "## <font color='blue'> Setup Working directory, copy files, execute scripts </font>\n",
    "<font color='blue'> <br>\n",
    "1) Copy input CSV files into your home/working directory <br>\n",
    "2) To perform web scraping, please copy the provided Chrome Driver into the working directory\n",
    "2) Copy this ipython (ipynb) notebook into your home/working directory <br>\n",
    "3) Ensure that the CSV files and this ipython notebook (ipynb) are in the home/working directory <br>\n",
    "4) Execute below scripts by sequence order </font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping from www.prokabaddi.com for Team information using Selenium Driver and Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to extract left part of the label\n",
    "def extract_left(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium  import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait as wait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    driver = webdriver.Chrome()\n",
    "    player_list=[]\n",
    "    driver.get(url)\n",
    "    wait(driver, 600).until(EC.presence_of_element_located((By.XPATH, \"//*[@class='si-data-block si-mwm']\")))\n",
    "    # or just wait for a second for browser(driver) to change\n",
    "    driver.implicitly_wait(100)\n",
    "    list1 = []\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    name_list = soup.findAll('div', attrs={'class': 'si-section-data si-left'})\n",
    "    cols_name_rows = name_list[0].findAll('div', attrs={'class': 'si-data-block'})\n",
    "\n",
    "    final_list = list(cols_name_rows)\n",
    "    driver.close();\n",
    "    return cols_name_rows\n",
    "#Define function to extract right part of the data\n",
    "def extract_right(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium  import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait as wait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    driver = webdriver.Chrome()\n",
    "    player_list=[]\n",
    "    driver.get(url)\n",
    "    wait(driver, 600).until(EC.presence_of_element_located((By.XPATH, \"//*[@class='si-data-block si-mwm']\")))\n",
    "    # or just wait for a second for browser(driver) to change\n",
    "    driver.implicitly_wait(100)\n",
    "    list1 = []\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "   \n",
    "    name_list = soup.findAll('div', attrs={'class': 'si-section-data si-right'})\n",
    "    cols = name_list[0].findAll('div', attrs={'class': 'si-data-block'})\n",
    "\n",
    "    final_list = list(cols)\n",
    "    driver.close();\n",
    "    return cols\n",
    "#Get label rows \n",
    "def label_column(cols_name_rows, seq):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import itertools\n",
    "    row_lists=[[1,2,3,4,5,6,7],[8,9,10,11,12,13,14,15],[18,19,20,21,22,23,24]]\n",
    "    main_list =[]\n",
    "\n",
    "    for row_list in row_lists:\n",
    "        player_list=[]\n",
    "        for row in row_list:\n",
    "        \n",
    "            soup1 = BeautifulSoup(str(cols_name_rows[row]),'lxml')\n",
    "\n",
    "            cols_1 = soup1.find('div', attrs={'class': 'si-data-block'}).text\n",
    "            player = cols_1.replace(\"\\n\",\"-\")\n",
    "            player = player.strip()\n",
    "            player_list.append(player)\n",
    "        main_list.append(player_list)\n",
    "        \n",
    "    #print(main_list)\n",
    "    #print(len(main_list))\n",
    "\n",
    "    merged_list = list(itertools.chain.from_iterable(main_list))\n",
    "    main_list[1].append('Total Raid Points')\n",
    "    main_list[2].insert(0,'Total Tackles')\n",
    "    df_stats1 = pd.DataFrame()\n",
    "    df_stats2 = pd.DataFrame()\n",
    "    df_stats3 = pd.DataFrame()\n",
    "    df_stats1['Stat_desc'] = pd.Series(main_list[0])\n",
    "    df_stats2['Stat_desc'] = pd.Series(main_list[1])\n",
    "    df_stats3['Stat_desc'] = pd.Series(main_list[2])\n",
    "    if (seq==1):\n",
    "        df_labels = pd.DataFrame()\n",
    "        df_labels = df_stats1\n",
    "        return df_labels\n",
    "    elif (seq==2):\n",
    "        df_labels = pd.DataFrame()\n",
    "        df_labels = df_stats2\n",
    "        return df_labels\n",
    "    elif (seq==3):\n",
    "        df_labels = pd.DataFrame()\n",
    "        df_labels = df_stats3\n",
    "        return df_labels\n",
    " \n",
    " #Get first section of data (as the data is split between various sections)\n",
    "def first_column(cols,season_ctrl):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import itertools\n",
    "    if (season_ctrl==7):\n",
    "        row_lists=[[8,9,10,11,12,13,14],[15,16,17,18,19,20,21],[22,23,24,25,26,27,28],[29,30,31,32,33,34,35],[36,37,38,39,40,41,42],\n",
    "          [43,44,45,46,47,48,49],[50,51,52,53,54,55,56],[57,58,59,60,61,62,63]]\n",
    "    else:\n",
    "        row_lists=[[4,5,6,7,8,9,10],[11,12,13,14,15,16,17],[18,19,20,21,22,23,24],[25,26,27,28,29,30,31]]\n",
    "    main_list =[]\n",
    "    for row_list in row_lists:\n",
    "        player_list=[]\n",
    "        for row in row_list:\n",
    "     \n",
    "            soup1 = BeautifulSoup(str(cols[row]),'lxml')\n",
    "            cols_1 = soup1.find('div', attrs={'class': 'si-data-block'}).text\n",
    "            player = cols_1.replace(\"\\n\",\"-\")\n",
    "            player = player.strip()\n",
    "            player_list.append(player)\n",
    "        main_list.append(player_list)\n",
    "   \n",
    "    #print(main_list)\n",
    "    #print(len(main_list))\n",
    "    #merged_list = list(itertools.chain.from_iterable(main_list))\n",
    " \n",
    "    df_overall = pd.DataFrame()\n",
    "    df_first = pd.DataFrame()\n",
    "    df_second = pd.DataFrame()\n",
    "    df_three = pd.DataFrame()\n",
    "    df_four = pd.DataFrame()\n",
    "    df_five = pd.DataFrame()\n",
    "    df_six = pd.DataFrame()\n",
    "    df_seven = pd.DataFrame()\n",
    "    if(season_ctrl==7):\n",
    "        df_overall['Overall'] = pd.Series(main_list[0])\n",
    "        df_first['Season1'] = pd.Series(main_list[7])\n",
    "        df_second['Season2'] = pd.Series(main_list[6])\n",
    "        df_three['Season3'] = pd.Series(main_list[5])\n",
    "        df_four['Season4'] = pd.Series(main_list[4])\n",
    "        df_five['Season5'] = pd.Series(main_list[3])\n",
    "        df_six['Season6'] = pd.Series(main_list[2])\n",
    "        df_seven['Season7'] = pd.Series(main_list[1])\n",
    "        frames = [df_overall, df_first, df_second,df_three,df_four,df_five,df_six,df_seven]\n",
    "        df_team_data2 = pd.concat(frames, axis=1)\n",
    "    else:\n",
    "        df_overall['Overall'] = pd.Series(main_list[0])\n",
    "        df_seven['Season7'] = pd.Series(main_list[1])\n",
    "        df_six['Season6'] = pd.Series(main_list[2])\n",
    "        df_five['Season5'] = pd.Series(main_list[3])\n",
    "        frames = [df_overall,df_five,df_six,df_seven]\n",
    "        df_team_data2 = pd.concat(frames, axis=1)\n",
    "    \n",
    "    df_team_result_final = df_team_data2\n",
    "    return df_team_result_final\n",
    "\n",
    "#Get second section of data (as the data is split between various sections)\n",
    "def second_column(cols,season_ctrl):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    if (season_ctrl==7):\n",
    "        my_list_ref = [64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267]\n",
    "    else:\n",
    "        my_list_ref = [32,33,34,35,36,37,38,39,40,41,41,43,44,45,46,47,48,49,50,51,51,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67]\n",
    "    n = 9 \n",
    "\n",
    "# using list comprehension \n",
    "    final = [my_list_ref[i * n:(i + 1) * n] for i in range((len(my_list_ref) + n - 1) // n )]  \n",
    "    row_lists = final[0:8]\n",
    "\n",
    "\n",
    "    main_list =[]\n",
    "\n",
    "    for row_list in row_lists:\n",
    "        player_list=[]\n",
    "        for row in row_list:\n",
    "        \n",
    "            soup1 = BeautifulSoup(str(cols[row]),'lxml')\n",
    "\n",
    "            cols_1 = soup1.find('div', attrs={'class': 'si-data-block'}).text\n",
    "            player = cols_1.replace(\"\\n\",\"-\")\n",
    "            player = player.strip()\n",
    "            player_list.append(player)\n",
    "        main_list.append(player_list)\n",
    "        \n",
    "    #print(main_list)\n",
    "    df_overall = pd.DataFrame()\n",
    "    df_first = pd.DataFrame()\n",
    "    df_second = pd.DataFrame()\n",
    "    df_three = pd.DataFrame()\n",
    "    df_four = pd.DataFrame()\n",
    "    df_five = pd.DataFrame()\n",
    "    df_six = pd.DataFrame()\n",
    "    df_seven = pd.DataFrame()\n",
    "    if(season_ctrl==7):\n",
    "        df_overall['Overall'] = pd.Series(main_list[0])\n",
    "        df_first['Season1'] = pd.Series(main_list[7])\n",
    "        df_second['Season2'] = pd.Series(main_list[6])\n",
    "        df_three['Season3'] = pd.Series(main_list[5])\n",
    "        df_four['Season4'] = pd.Series(main_list[4])\n",
    "        df_five['Season5'] = pd.Series(main_list[3])\n",
    "        df_six['Season6'] = pd.Series(main_list[2])\n",
    "        df_seven['Season7'] = pd.Series(main_list[1])\n",
    "        frames = [df_overall, df_first, df_second,df_three,df_four,df_five,df_six,df_seven]\n",
    "        df_team_data2 = pd.concat(frames, axis=1)\n",
    "    else:\n",
    "        df_overall['Overall'] = pd.Series(main_list[0])\n",
    "        df_seven['Season7'] = pd.Series(main_list[1])\n",
    "        df_six['Season6'] = pd.Series(main_list[2])\n",
    "        df_five['Season5'] = pd.Series(main_list[3])\n",
    "        frames = [df_overall,df_five,df_six,df_seven]\n",
    "        df_team_data2 = pd.concat(frames, axis=1)\n",
    "    df_team_result_final = df_team_data2\n",
    "    return df_team_result_final\n",
    "\n",
    "#Get third section of data (as the data is split between various sections)\n",
    "def third_column(cols,season_ctrl):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    if(season_ctrl==7):\n",
    "        my_list_ref = [136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,146 ,147 ,148 ,149 ,150 ,151 ,152 ,153 ,154 ,155 ,156 ,157 ,158 ,159 ,160 ,161 ,162 ,163 ,164 ,165 ,166 ,167 ,168 ,169 ,170 ,171 ,172 ,173 ,174 ,175 ,176 ,177 ,178 ,179 ,180 ,181 ,182 ,183 ,184 ,185 ,186 ,187 ,188 ,189 ,190 ,191 ,192 ,193 ,194 ,195 ,196 ,197 ,198 ,199 ,200 ,201 ,202 ,203 ,204 ,205 ,206 ,207 ,208 ,209 ,210 ,211 ,212 ,213 ,214 ,215 ,216 ,217 ,218 ,219 ,220 ,221 ,222 ,223 ,224 ,225 ,226 ,227 ,228 ,229 ,230 ,231 ,232 ,233 ,234 ,235 ,236 ,237 ,238 ,239 ,240 ,241 ,242 ,243 ,244 ,245 ,246 ,247 ,248 ,249 ,250 ,251 ,252 ,253 ,254 ,255 ,256 ,257 ,258 ,259 ,260 ,261 ,262 ,263 ,264 ,265 ,266 ,267 ,268 ,269 ,270 ,271 ,272 ,273 ,274 ,275 ,276 ,277 ,278 ,279 ,280 ,281 ,282 ,283 ,284 ,285 ,286 ,287 ,288 ,289 ,290 ,291 ,292 ,293 ,294 ,295 ,296 ,297 ,298 ,299 ,300 ,301 ,302 ,303 ,304 ,305 ,306 ,307 ,308 ,309 ,310 ,311 ,312 ,313 ,314 ,315 ,316 ,317 ,318 ,319 ,320 ,321 ,322 ,323 ,324 ,325 ,326 ,327 ,328 ,329]\n",
    "    else:\n",
    "        my_list_ref = [68 ,69 ,70 ,71 ,72 ,73 ,74 ,75 ,76 ,77 ,78 ,79 ,80 ,81 ,82 ,83 ,84 ,85 ,86 ,87 ,88 ,89 ,90 ,91 ,92 ,93 ,94 ,95,96,97,98,99]\n",
    "    n = 8 \n",
    "\n",
    "# using list comprehension \n",
    "    final = [my_list_ref[i * n:(i + 1) * n] for i in range((len(my_list_ref) + n - 1) // n )]  \n",
    "    row_lists = final[0:8]\n",
    "    main_list =[]\n",
    "    for row_list in row_lists:\n",
    "        player_list=[]\n",
    "        for row in row_list:\n",
    "            soup1 = BeautifulSoup(str(cols[row]),'lxml')\n",
    "            cols_1 = soup1.find('div', attrs={'class': 'si-data-block'}).text\n",
    "            player = cols_1.replace(\"\\n\",\"-\")\n",
    "            player = player.strip()\n",
    "            player_list.append(player)\n",
    "        main_list.append(player_list)\n",
    "       \n",
    "    #print(main_list)\n",
    "    df_overall = pd.DataFrame()\n",
    "    df_first = pd.DataFrame()\n",
    "    df_second = pd.DataFrame()\n",
    "    df_three = pd.DataFrame()\n",
    "    df_four = pd.DataFrame()\n",
    "    df_five = pd.DataFrame()\n",
    "    df_six = pd.DataFrame()\n",
    "    df_seven = pd.DataFrame()\n",
    "\n",
    "    if(season_ctrl==7):\n",
    "        df_overall['Overall'] = pd.Series(main_list[0])\n",
    "        df_first['Season1'] = pd.Series(main_list[7])\n",
    "        df_second['Season2'] = pd.Series(main_list[6])\n",
    "        df_three['Season3'] = pd.Series(main_list[5])\n",
    "        df_four['Season4'] = pd.Series(main_list[4])\n",
    "        df_five['Season5'] = pd.Series(main_list[3])\n",
    "        df_six['Season6'] = pd.Series(main_list[2])\n",
    "        df_seven['Season7'] = pd.Series(main_list[1])\n",
    "        frames = [df_overall, df_first, df_second,df_three,df_four,df_five,df_six,df_seven]\n",
    "        df_team_data2 = pd.concat(frames, axis=1)\n",
    "    else:\n",
    "        df_overall['Overall'] = pd.Series(main_list[0])\n",
    "        df_seven['Season7'] = pd.Series(main_list[1])\n",
    "        df_six['Season6'] = pd.Series(main_list[2])\n",
    "        df_five['Season5'] = pd.Series(main_list[3])\n",
    "        frames = [df_overall,df_five,df_six,df_seven]\n",
    "        df_team_data2 = pd.concat(frames, axis=1)\n",
    "    df_team_result_final = df_team_data2\n",
    "    return df_team_result_final\n",
    "\n",
    "#pass the url to get the dataframe\n",
    "def get_final_df(url,season_ctrl):\n",
    "    cols = extract_left(url)\n",
    "    df_label1 = label_column(cols,1)\n",
    "    df_label2 = label_column(cols,2)\n",
    "    df_label3 = label_column(cols,3)\n",
    "    cols1 = extract_right(url)\n",
    "    df_col1 = first_column(cols1,season_ctrl)\n",
    "    df_col2 = second_column(cols1,season_ctrl)\n",
    "    df_col3 = third_column(cols1,season_ctrl)\n",
    "    df3 = pd.concat([df_label1,df_col1], axis=1)\n",
    "    df4 = pd.concat([df_label2,df_col2], axis=1)\n",
    "    df5 = pd.concat([df_label3,df_col3], axis=1)\n",
    "    df6 = pd.concat([df3,df4], axis=0)\n",
    "    df7 = pd.concat([df6,df5], axis=0)\n",
    "    return df7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling main function to extract data from the given URLs\n",
    "urls = ['https://www.prokabaddi.com/teams/bengal-warriors-profile-4','https://www.prokabaddi.com/teams/bengaluru-bulls-profile-1',\n",
    "'https://www.prokabaddi.com/teams/dabang-delhi-kc-profile-2','https://www.prokabaddi.com/teams/gujarat-fortunegiants-profile-31',\n",
    "'https://www.prokabaddi.com/teams/haryana-steelers-profile-28','https://www.prokabaddi.com/teams/jaipur-pink-panthers-profile-3',\n",
    "'https://www.prokabaddi.com/teams/patna-pirates-profile-6','https://www.prokabaddi.com/teams/puneri-paltan-profile-7',\n",
    "'https://www.prokabaddi.com/teams/tamil-thalaivas-profile-29','https://www.prokabaddi.com/teams/telugu-titans-profile-8',\n",
    "'https://www.prokabaddi.com/teams/u-mumba-profile-5','https://www.prokabaddi.com/teams/up-yoddha-profile-30']\n",
    "for url in urls:\n",
    "    str1 = url.rsplit('/', 1)[-1]\n",
    "    str2 = str1.split('-profile')\n",
    "    str3 = str2[0]\n",
    "    season_ctrl = 7\n",
    "    if (str3=='gujarat-fortunegiants'):\n",
    "        season_ctrl=3\n",
    "    elif (str3=='haryana-steelers'):\n",
    "        season_ctrl=3\n",
    "    elif (str3=='tamil-thalaivas'):\n",
    "        season_ctrl=3\n",
    "    elif (str3=='up-yoddha'):\n",
    "        season_ctrl=3\n",
    "    df_final = get_final_df(url,season_ctrl)\n",
    "    df_final['Team'] = str3\n",
    "    file_name = str3\n",
    "    file_name = file_name +'.csv'\n",
    "    df_final.to_csv(file_name,index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Web Scraping for Team data. Check the working directory for the CSV output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
